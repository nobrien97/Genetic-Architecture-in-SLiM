---
title: "Speed Tests"
author: "Nick O'Brien"
date: "20/07/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## SLiM Speed

Now that my models are approaching completion, I am needing to figure out exactly how long everything will take to run and optimise as much as possible. From my previous tests (and the SLiM manual) I've found that population size and recombination contribute the most to run time, but I've also now found that my stabilising selection model has a huge performance impact. To find this, I ran two jobs on Tinaroo: simple scripts that ran one 8 trait null or stabilising selection model with worst case scenario parameters (0.5 recombination between regions, high population size, large recombination rate etc., all pleiotropic mutations). That script is below:

```{r speedtestscript, eval=FALSE}

# Worst case scenario test run for 8T Null

system.time(system("/home/$USER/SLiM/slim -s 123 -d rregion=0.5 -d Ne=10000 -d QTL_cov=0.5 -d pleiorate=1.0 -d delmu=0.0 -d rwide=1.346e-5 /home/$USER/SLiM/Scripts/null8T100L_nowrite.slim"))


# Worst case scenario test run for 8T Selection

system.time(system("/home/$USER/SLiM/slim -s 123 -d rregion=0.5 -d Ne=10000 -d QTL_cov=0.5 -d pleiorate=1.0 -d delmu=0.0 -d rwide=1.346e-5 /home/$USER/SLiM/Scripts/stabsel_recom_8T100L_nowrite.slim"))


```

## Results

I found that the null model with worst case scenario parameters takes 26655.862 seconds, or ~7.41 hours.
This is fine - I'll be able to run heaps of things,
The stabilising selection model is more concerning - it didn't finish in the 24 hours I gave it, so I may have to be very careful with how many runs I do. If one model takes a week that means 5 will take 5 weeks. If we're running on enough cores this may not be a problem, but if we're doing 10,000 models and we want to take ~4 weeks to run everything, that would be 2500 cores we'd need, which is way may than the system can reasonably provide (I think anyway - that's close to half of the total cores available on the system, 5856). 

I'll need to run this thing to its finish state to determine how long it will take - if it's well under a week per model it should be fine, otherwise I'll have to consider what can be done to improve performance.
Since SLiM's variance and covariance functions don't support matrix inputs, I can't vectorise my phenotypes, which is a huge pain and may have been a way to get some speed.  



## Optimisation

*Update 20/7 4pm* I've made some minor changes to the fitness calculation which has improved the speed somewhat, although it is still relatively slow. Here is the old way of doing things:

```{rcpp oldway, eval = FALSE}
	muts = ind.genomes.mutationsOfType(m3);			// Get only pleiotropic mutations first, then add the other effects later
		mutscount = size(muts);
		
		phenotype0 = (mutscount ? sum(muts.getValue("e0")) else 0.0);
		phenotype0 = phenotype0 + (ind.sumOfMutationsOfType(m4)) + (fixedfx0); // add non-pleiotropic effects and fixed effects
		
		phenotype1 = (mutscount ? sum(muts.getValue("e1")) else 0.0);
		phenotype1 = phenotype1 + (ind.sumOfMutationsOfType(m5)) + (fixedfx1); // add non-pleiotropic effects and fixed effects
		
		phenotype2 = (mutscount ? sum(muts.getValue("e2")) else 0.0);
		phenotype2 = phenotype2 + (ind.sumOfMutationsOfType(m6)) + (fixedfx2); // add non-pleiotropic effects and fixed effects
		
		phenotype3 = (mutscount ? sum(muts.getValue("e3")) else 0.0);
		phenotype3 = phenotype3 + (ind.sumOfMutationsOfType(m7)) + (fixedfx3); // add non-pleiotropic effects and fixed effects
		
		phenotype4 = (mutscount ? sum(muts.getValue("e4")) else 0.0);
		phenotype4 = phenotype4 + (ind.sumOfMutationsOfType(m8)) + (fixedfx4); // add non-pleiotropic effects and fixed effects
		
		phenotype5 = (mutscount ? sum(muts.getValue("e5")) else 0.0);
		phenotype5 = phenotype5 + (ind.sumOfMutationsOfType(m9)) + (fixedfx5); // add non-pleiotropic effects and fixed effects
		
		phenotype6 = (mutscount ? sum(muts.getValue("e6")) else 0.0);
		phenotype6 = phenotype6 + (ind.sumOfMutationsOfType(m10)) + (fixedfx6); // add non-pleiotropic effects and fixed effects
		
		phenotype7 = (mutscount ? sum(muts.getValue("e7")) else 0.0);
		phenotype7 = phenotype7 + (ind.sumOfMutationsOfType(m11)) + (fixedfx7); // add non-pleiotropic effects and fixed effects
		
		ind.setValue("phenotype0", phenotype0);
		ind.setValue("phenotype1", phenotype1);
		ind.setValue("phenotype2", phenotype2);
		ind.setValue("phenotype3", phenotype3);
		ind.setValue("phenotype4", phenotype4);
		ind.setValue("phenotype5", phenotype5);
		ind.setValue("phenotype6", phenotype6);
		ind.setValue("phenotype7", phenotype7);
		
// Fitness calculated using dnorm: Gaussian function around the optimum phenotype, with the probability density of the phenotype	corresponding to the fitness 		
// 	Measuring how far fitness is from optimum as the quantile, then gets the prob density at said quantile from sd 0, 10; multiplies by a selection strength
// dnorm function: first is the quantile (optima minus the individual's phenotype value), mean of the distribution is 0, variance is equal to the optima value (so 1 standard deviation of the optimum)
// So the larger the difference between phenotype and optimum, the lower the fitness (as you are further from 0)

		
		effect0 = 1.0 + dnorm(optima[0] - phenotype0, 0, abs(optima[0])) * 1000.0;		// From SLiM Manual p.225 + Online Workshop 13; width of 20 hard coded for trhe fitness functions, but optima are taken from the optima values defined earlier. Last number is a multiplier so that individuals precisely at the optimum have fitness x times greater than those infinitely far from the optima
		effect1 = 1.0 + dnorm(optima[1] - phenotype1, 0, abs(optima[1])) * 1000.0;		// this term, dnorm()*x is our selection coefficient, s, effect is w
		effect2 = 1.0 + dnorm(optima[2] - phenotype2, 0, abs(optima[2])) * 1000.0;		// except this applies to an individual's fitness over another rather than
		effect3 = 1.0 + dnorm(optima[3] - phenotype3, 0, abs(optima[3])) * 1000.0;		// a genotype's
		effect4 = 1.0 + dnorm(optima[4] - phenotype4, 0, abs(optima[4])) * 1000.0;		
		effect5 = 1.0 + dnorm(optima[5] - phenotype5, 0, abs(optima[5])) * 1000.0;		
		effect6 = 1.0 + dnorm(optima[6] - phenotype6, 0, abs(optima[6])) * 1000.0;		
		effect7 = 1.0 + dnorm(optima[7] - phenotype7, 0, abs(optima[7])) * 1000.0;		
		ind.fitnessScaling = effect0 + effect1 + effect2 + effect3 + effect4 + effect5 + effect6 + effect7;	// Since we are multiplying everything, this is like stab sel through correlational selection
```

As you can see, we are spreading everything out for visibility and clarity, but this results in a bit of slowdown. As well as the massive for loop, we've got pleitropic phenotype effects separated from regular mutation effects, which is slow because Eidos has to grab the phenotype from memory and then add stuff to it. The new way is way more confusing to read, but does essentially the same thing:

```{rcpp newway, eval=FALSE}

	phenotype = float(8);
	phenotype = sapply(inds, <<
	muts = applyValue.genomes.mutationsOfType(m3); mutscount = size(muts); phenotype[0] = (mutscount ? sum(muts.getValue("e0")) else 0.0) + (applyValue.sumOfMutationsOfType(m4) + fixedfx0); phenotype[1] = (mutscount ? sum(muts.getValue("e1")) else 0.0) + (applyValue.sumOfMutationsOfType(m5) + fixedfx1); phenotype[2] = (mutscount ? sum(muts.getValue("e2")) else 0.0) + (applyValue.sumOfMutationsOfType(m6) + fixedfx2); phenotype[3] = (mutscount ? sum(muts.getValue("e3")) else 0.0) + (applyValue.sumOfMutationsOfType(m7) + fixedfx3); phenotype[4] = (mutscount ? sum(muts.getValue("e4")) else 0.0) + (applyValue.sumOfMutationsOfType(m8) + fixedfx4); phenotype[5] = (mutscount ? sum(muts.getValue("e5")) else 0.0) + (applyValue.sumOfMutationsOfType(m9) + fixedfx5); phenotype[6] = (mutscount ? sum(muts.getValue("e6")) else 0.0) + (applyValue.sumOfMutationsOfType(m10) + fixedfx6);		 phenotype[7] = (mutscount ? sum(muts.getValue("e7")) else 0.0) + (applyValue.sumOfMutationsOfType(m11) + fixedfx7); applyValue.setValue("phenotype0", phenotype[0]); applyValue.setValue("phenotype1", phenotype[1]); applyValue.setValue("phenotype2", phenotype[2]); applyValue.setValue("phenotype3", phenotype[3]); applyValue.setValue("phenotype4", phenotype[4]); applyValue.setValue("phenotype5", phenotype[5]); applyValue.setValue("phenotype6", phenotype[6]); applyValue.setValue("phenotype7", phenotype[7]); effect = float(8); for (i in phenotype) {	effect[match(i, phenotype)] = (1.0 + dnorm(optima[match(i, phenotype)] - i, 0, abs(optima[match(i, phenotype)])) * 100.0); } applyValue.fitnessScaling = sum(effect); phenotype;
>>;
```

Here I've scrapped the for loop in favour of a sapply() 'loop', and joined together the separated phenotype calculations. I also made phenotype a vector of length 8 to store all 8 traits in one variable rather than having to fish through 8 different parts of memory to get the appropriate variable.
I also lowered the fitness multiplier to 100, which is more realistic according to the SLiM manual.

I still need to do a bit of testing regarding this to ensure it is the same behaviour as before, but I'm reasonably confident it works as intended.



## Update 3/8/2020

After implementing my heterozygosity-based burn-in, and making some structural changes to the model (all models now have 10 chromosomes, recombination is more realistic and varies between chromosomes, varied deleterious mutations to occur on a certain number of chromosomes), I realised all models were running a bit slower than before. I did a test run of null and recom, and found a substantial difference in time to completion. Before, the null model was at 7.41 hours for a single run. This ballooned to ~45.2 hours following these chances. Similarly, AIM2's model went from 8.40 hours to 47.01 hours. Now, these models are running for an additional 50,000 generations, but this is still considerably higher than predicted than my older efforts to predict time to run. 
Given both of these models increased in time in a similar way (a similar scale), it could be expected that the selection model will as well. However, the major problem with the selection model is the fitness function, and with more loci to add (a new treatment), this is likely to not scale in the same way. As a result, I'll need to run another test for the selection model to see how long it takes.

For AIM1: Total time to run is 452000 CPU hours (10000 runs). Clearly, we will need a pretty large number of nodes to do this properly: with 100 nodes, we get - 452000/(100*24) = 188.33 hrs, or 7.85 days.

For AIM2: Total time to run is 470100 CPU hours (10000 runs). With 100 nodes, we get 470100/(100*24) = 195.875 hrs or 8.16 days.

If AIM3 scaled in the same way as the other two, that means it would take 416.38 hours (17.35 days!) to run - way too long I think. With 100 nodes, that would be 72.28 days of running. If we scale our models down to 50 replicates for that one, we halve the time to 36 days which might be achievable. Ideally, we could get it below a month of run-time but without a huge amount of processing power or massive cuts to the number of replicates/models I'm not sure if that is possible.


---
title: "May 2020 Model Improvements"
author: "Nick O'Brien"
date: "08/06/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

This document details the improvements and adjustments I have made to my SLiM models based on further research and understanding of the Eidos language.

I looked at implementing pleiotropic mutations with a particular prevalence (available as a command line parameter), optimisations of mutations by allowing them to become substitutions whilst still affecting phenotype, and an additional output for mutations.

I also implemented a Latin hypercube sampling technique to examine the entire parameter space more efficiently, now that the parameter space is growing. I'm using the DoE.wrapper package in R, which uses both LHS and DiceDesign to generate LHS combinations in a more user-friendly configuration, including differing ranges for parameters. The parameters I included are population size (Ne), proportion of deleterious mutation (delmu), proportion of pleiotropic mutation (pleiorate), recombination rate between genomic regions, a measure of linkage (rregion), recombination rate across the whole genome (aside from between those regions) (rwide), and the covariance used in the sigma used for the multivariate normal distribution when pulling pleiotropic effects, which is a measure of pleiotropic covariance or correlation (QTL_cov). 

I was also able to use Awoonga to run simulations on a computing cluster, utilising an R script to run SLiM in parallel. I will soon move to using Tinaroo so I can access more cores, which will greatly improve processing speed as I increase the runtime of my models (in terms of generations), and enable me to explore wider parameter spaces.

Firstly, I will explain my implementation of pleiotropic mutations:

```{rcpp pleiotropy1, eval = FALSE}
initialize() {
	setCfgParam("QTL_mu", c(0, 0)); // mean distribution value for multivariate normal we are pulling effects of both traits from
	setCfgParam("QTL_cov", 0.0); // covariance between the pulls in the multivariate normal (pleiotropic correlation between traits)
	setCfgParam("QTL_sigma", matrix(c(1, QTL_cov, QTL_cov, 1), nrow=2)); // sigma used in multivariate normal pull - variance covariance matrix, where variance of each trait = 1
	
	catn("\nQTL DFE means: ");
	print(QTL_mu);
	catn("\nQTL DFE variance-covariance matrix (M matrix): ");
	print(QTL_sigma);
}

mutation(m3) {
	// Draw mutational effects for the m3 mutation
	effects = rmvnorm(1, QTL_mu, QTL_sigma);
	mut.setValue("e0", effects[0]);
	mut.setValue("e1", effects[1]);
	mut.setValue("fe", 0.0); //fitness effect
	
	return T;

// For non-pleiotropic mutations, set their e0 and e1 values so we can have standardised output in that format

mutation(m2) {
	mut.setValue("e0", 0.0);
	mut.setValue("e1", 0.0);
	mut.setValue("fe", mut.selectionCoeff); // fitness effect
	
	return T;

}

mutation(m4) {
	mut.setValue("e0", mut.selectionCoeff);
	mut.setValue("e1", 0.0);
	mut.setValue("fe", 0.0); //fitness effect
	
	return T;

}

mutation(m5) {
	mut.setValue("e0", 0.0);
	mut.setValue("e1", mut.selectionCoeff);
	mut.setValue("fe", 0.0); //fitness effect
	
	return T;

}
	

1:50000 late() {
	
	if (sim.generation % 250 != 0) // !CHECK THIS! Grab a sample every 500 generations: results in 200 time points per run, just like old model, file size should be the same as before
		return;
	// For each individual, grab the mutation effects and store them as a phenotype value for each phenotype, summing the effects of m3, m4, and/or, m5. This is additive!
	
	// First, collect the substitutions to apply to all individuals' phenotypes
	inds = sim.subpopulations.individuals;
	fixed = sim.substitutions;
	sizem3 = size(fixed[fixed.mutationType == m3]);
	sizem4 = size(fixed[fixed.mutationType == m4]);
	sizem5 = size(fixed[fixed.mutationType == m5]);
	
	fixedfx0 = sizem3 ? sum(fixed[fixed.mutationType == m3].getValue("e0")) else 0.0 + sizem4 ? sum(fixed[fixed.mutationType == m4].getValue("e0")) else 0.0;
	
	fixedfx1 = sizem3 ? sum(fixed[fixed.mutationType == m3].getValue("e1")) else 0.0 + sizem5 ? sum(fixed[fixed.mutationType == m5].getValue("e1")) else 0.0;
	
	
	for (ind in sim.subpopulations.individuals)
	{
		muts = ind.genomes.mutationsOfType(m3);
		mutscount = size(muts);
		
		phenotype0 = (mutscount ? sum(muts.getValue("e0")) else 0.0);
		phenotype0 = phenotype0 + (ind.sumOfMutationsOfType(m4)) + (fixedfx0); // add non-pleiotropic effects and fixed effects
		
		phenotype1 = (mutscount ? sum(muts.getValue("e1")) else 0.0);
		phenotype1 = phenotype1 + (ind.sumOfMutationsOfType(m5)) + (fixedfx1); // add non-pleiotropic effects and fixed effects
		
		ind.setValue("phenotype0", phenotype0);
		ind.setValue("phenotype1", phenotype1);
	}
	// Get the values for output: mean trait values, variance of those trait values, covariance and correlation coefficients
	inds = sim.subpopulations.individuals;
	pheno0mean = mean(inds.getValue("phenotype0"));
	pheno1mean = mean(inds.getValue("phenotype1"));
	pheno0var = var(inds.getValue("phenotype0"));
	pheno1var = var(inds.getValue("phenotype1"));
	phenocov = cov(inds.getValue("phenotype0"), inds.getValue("phenotype1"));
	phenocor = cor(inds.getValue("phenotype0"), inds.getValue("phenotype1"));
}
```

The above code explains my implementation of pleiotropic mutations (m3), based off the recipes given in the SLiM online workshop and manual. However, I have extended it to include other mutation types which are non-pleiotropic and affect the phenotypes separately, with the rate of pleiotropy being a proportion that is configurable in script as a cfgparam. 

The pleiotropic effects are pulled from a multivariate normal distribution with means = 0, and a sigma defined with variances of 1 and a covariance which is set as a parameter to be adjusted. This covariance is a measure of the directionality of pleiotropy, and the strength of that correlation, where values further away in either direction from 0 bias the multivariate pulls to be positively or negatively correlated due to pleiotropic correlation. This is altered by changes to genetic architecture (the other parameters apart from Ne) and evolutionary processes such as drift (Ne) (and probably selection, which I have not incorporated yet). 

The individual effects (no pleiotropy) are pulled from standard normal distributions, and deleterious mutations from a gamma distribution with mean -0.03 and shape 0.2. The default chance of a given mutation being pleiotropic vs any other type of mutation is 44%, which comes from Chesmore et al. 2017, a study assessing the level of pleiotropy in human disease mutations. This parameter is exposed to the command line so it can be included in hypercube sampling to sample the entire space (from 0 to 1) - as pleiotropic mutations become more prevalent, what happens to variance etc.

When I add more traits, I will need to include another parameter - the mean number of traits that are affected by a single mutation. Chesmore et al. 2017 found that most pleiotropic mutations only affected 2 traits, with a seemingly normal drop-off for larger number of traits, with one mutation affecting ~50 traits! As a result, I expect to use a folded normal distribution (I'll get the absolute value from a normal pull) to generate the number of traits that a given mutation will affect. This distribution will have a mean of 2 and a variance that I have yet to figure out; I will do more reading into this to determine what would be most realistic. I can use the mean number of traits (and also the variance) in this normal pull as parameters to change in my parameter space as well. Of course, I will need to round these values as well, so we aren't somehow affecting 2.5 traits. I'll also need to make sure that if a number is pulled that would be rounded to less than 2, it gets resampled, as the mutation won't be pleiotropic if it only affects 1 trait. 


## Substitution conversion


```{rcpp substitutions, eval=FALSE}
	// First, collect the substitutions to apply to all individuals' phenotypes
	inds = sim.subpopulations.individuals;
	fixed = sim.substitutions;
	sizem3 = size(fixed[fixed.mutationType == m3]);
	sizem4 = size(fixed[fixed.mutationType == m4]);
	sizem5 = size(fixed[fixed.mutationType == m5]);
	
	fixedfx0 = sizem3 ? sum(fixed[fixed.mutationType == m3].getValue("e0")) else 0.0 + sizem4 ? sum(fixed[fixed.mutationType == m4].getValue("e0")) else 0.0;
	
	fixedfx1 = sizem3 ? sum(fixed[fixed.mutationType == m3].getValue("e1")) else 0.0 + sizem5 ? sum(fixed[fixed.mutationType == m5].getValue("e1")) else 0.0;
	
	
	for (ind in sim.subpopulations.individuals)
	{
		muts = ind.genomes.mutationsOfType(m3);
		mutscount = size(muts);
		
		phenotype0 = (mutscount ? sum(muts.getValue("e0")) else 0.0);
		phenotype0 = phenotype0 + (ind.sumOfMutationsOfType(m4)) + (fixedfx0); // add non-pleiotropic effects and fixed effects
		
		phenotype1 = (mutscount ? sum(muts.getValue("e1")) else 0.0);
		phenotype1 = phenotype1 + (ind.sumOfMutationsOfType(m5)) + (fixedfx1); // add non-pleiotropic effects and fixed effects
		
		ind.setValue("phenotype0", phenotype0);
		ind.setValue("phenotype1", phenotype1);
	}
```

I noticed that run times where slowing down towards the ends of runs as I increased the generation limit, and hypothesised that this was due to a buildup of mutations that had fixed in the population. So I worked on converting the model to allow for substitutions, while retaining the phenotype effects of the substitutions. Fortunately, SLiM retains values (created using the setValue() method) for mutations that have been converted to substitutions, so all that required was adding the values from substitution objects to the phenotype as well as from mutations. Since I now have mutations that are non-pleiotropic for the traits, I also needed to add their effects, in a similar way to the simplem quantitative trait workshop, using sumOfMutationsOfType().

So far this phenotype calculation is only done when output is generated, however in my selection models, I will need to calculate it each generation because fitness will be dependent on phenotype, probably in multiple directions. Most of these calculations are vectorised already so it should be fast enough, but we will definitely need to use Tinaroo rather than Awoonga for this, the extra cores will be necessary.


## Mutation output

Here is my new output for mutations, which is taken at this point every 2500 generations, or once for every 10 samples of population means. This is done because of file size: each line in this output is another mutation, so there are often many more lines per generation than in the population-aggregate output. This of course can be adjusted and will likely need to be, but we'll see how large this file gets with more complex runs.

```{rcpp mutationoutput, eval=FALSE}
// Mutation output: only do this once every 2500 generations: missing a lot of micro-evolution, but we keep fixations, and the files get too big otherwise	
	
	if (sim.generation % 2500 != 0)
		return;
	
	// Output for mutations
	
	// Store vectors of unique mutations, sorted by the generation in which they originated
	muts = sortBy(c(sim.mutationsOfType(m2), sim.mutationsOfType(m3), sim.mutationsOfType(m4), sim.mutationsOfType(m5)), "originGeneration");
	
	// same for substitutions
	subs = sortBy(c(sim.substitutions[sim.substitutions.mutationType != m1]), "originGeneration");
	
	// Set up an empty output to store all our mutations' information for this generation
	mutsLines = NULL;
	revmuts = rev(muts); // Reverse of the vector: the first value in this is the last mutation in the original list
	revsubs = rev(subs); // could instead call muts.size() and use if mut == muts[muts.size() - 1] to check, maybe uses less memory, probably same speed
	
	// Put information on these unique mutations into a separate mutations file	
	
	// Check if there are any substitutions, if not we need to fix the output lines	by not including the \n character at the end of the last mutation line
	
	if (size(subs))
		for (mut in muts) {
			mutFreq = mean(sim.subpopulations.genomes.containsMutations(mut));
			mutType = mut.mutationType.id;
			mutsLine = paste(c(sim.generation, ", ", getSeed(), ", ", modelindex, ", ", rregion, ", ", rwide, ", ", delmu, ", ", QTL_cov, ", ", Ne, ", ", mutType, ", ", mut.id, ", ", mut.position, ", ", mut.originGeneration, ", ", mut.getValue("e0"), ", ", mut.getValue("e1"), ", ", mut.getValue("fe"), ", ", mutFreq, ", ", "N/A", "\n"), "");
			
			mutsLines = c(mutsLines, mutsLine);
		}
	else
		for (mut in muts) {
			mutFreq = mean(sim.subpopulations.genomes.containsMutations(mut));
			mutType = mut.mutationType.id;
			if (mut != revmuts[0]) // Quality hack
				mutsLine = paste(c(sim.generation, ", ", getSeed(), ", ", modelindex, ", ", rregion, ", ", rwide, ", ", delmu, ", ", QTL_cov, ", ", Ne, ", ", mutType, ", ", mut.id, ", ", mut.position, ", ", mut.originGeneration, ", ", mut.getValue("e0"), ", ", mut.getValue("e1"), ", ", mut.getValue("fe"), ", ", mutFreq, ", ", "N/A", "\n"), "");
			else
				mutsLine = paste(c(sim.generation, ", ", getSeed(), ", ", modelindex, ", ", rregion, ", ", rwide, ", ", delmu, ", ", QTL_cov, ", ", Ne, ", ", mutType, ", ", mut.id, ", ", mut.position, ", ", mut.originGeneration, ", ", mut.getValue("e0"), ", ", mut.getValue("e1"), ", ", mut.getValue("fe"), ", ", mutFreq, ", ", "N/A", ""), "");
			mutsLines = c(mutsLines, mutsLine);
		}
	
	
	
	for (sub in subs) {
		subFreq = 1.0;
		subType = sub.mutationType.id;
		
		if (sub != revsubs[0]) // THIS IS THE HACK THAT I MUST RESORT TO THANKS TO NO \b OR REPLACE() 	GOSH BLESS EIDOS
			subLine = paste(c(sim.generation, ", ", getSeed(), ", ", modelindex, ", ", rregion, ", ", rwide, ", ", delmu, ", ", QTL_cov, ", ", Ne, ", ", subType, ", ", sub.id, ", ", sub.position, ", ", sub.originGeneration, ", ", sub.getValue("e0"), ", ", sub.getValue("e1"), ", ", sub.getValue("fe"), ", ", subFreq, ", ", sub.fixationGeneration, "\n"), "");
		else
			subLine = paste(c(sim.generation, ", ", getSeed(), ", ", modelindex, ", ", rregion, ", ", rwide, ", ", delmu, ", ", QTL_cov, ", ", Ne, ", ", subType, ", ", sub.id, ", ", sub.position, ", ", sub.originGeneration, ", ", sub.getValue("e0"), ", ", sub.getValue("e1"), ", ", sub.getValue("fe"), ", ", subFreq, ", ", sub.fixationGeneration, ""), "");
		
		mutsLines = c(mutsLines, subLine); // Add substitutions to mutation output
	
	}
	
	mutsFile = paste(mutsLines, ""); // n.b. does \b exist in eidos? Would make it easier
	
	// Write the mutations file
	// Fail safe in case there are no mutations at some point and the for loop doesn't run
	if (exists('mutsLine'))
		writeFile(outnamemuts, mutsFile, append = T);
	
	
	// SLiMGUI test output 	
	//	catn(paste(c("Generation: ", sim.generation, " Phenotype 0 Mean: ", pheno0mean, " Phenotype 1 Mean: ", pheno1mean, " covariance: ", phenocov)));

}
```

The way this works is by tracking each mutation with an effect on fitness and/or phenotype. The output gives a description of the model itself in the same way as the population output (the treatments, and a new modelindex parameter which is the parameter combination in the list of hypercube runs that it is, which allows for nesting in analysis), the type of mutation (m2, m3, m4, m5: m2 is deleterious, m3 is pleiotropic, m4 and m5 affect only trait 1 or 2, respectively), the SLiM identifier for the mutation (no biological meaning, just confirms a mutation is the same one if we've got it in another generation), the position on the chromosome, the origin generation, phenotype effects, fitness effects (0 for everything except m2), the frequency, and (if applicable), the fixation generation.

The output format is a little funky: it is done on a mutation by mutation basis, then a substitution by substitution basis - for each mutation/subsitution, it grabs information (as above) from that object and puts it in a new line, then adds a new line to prepare for the next mutation UNLESS there are either no substitutions and the mutation is the last in the list, or it is the last subsitution in the list. The reason for this is that the writeFile() function automatically adds a new line when using append = T, and there is no way to get around this. My initial plans to solve this problem (which was resulting in an extra blank line between each output, rather annoyingly) was to use the backspace escape sequence to remove the final extra line once the entire sequence of mutations was written, which doesn't exist in Eidos despite the new line escape sequence being there. So my next option was to detect the last run of my for loop to determine which mutation should not have a new line added to it (since writeFile() was going to do it anyway). My first instinct was to use some kind of .Last() method, a la C, but it didn't exist. There were two options: get the length of the list of mutations and check if the current iteration (mut) is length - 1, or store the reverse of the vector storing the mutations and check if the current iteration is reversevector[0] (if it is, it is the last mutation, so we shouldn't add the new line). Both methods are pretty fast as far as I can tell; the length() way is probably slightly less memory intensive, but the reverse way is cooler, so I kept that for this iteration, although I will likely change it later on to use length - 1.

## Latin Hypercube Sampling and populations of models

I'm now looking at combinations of parameters rather than each one in a factorial design, due to the scale of the parameter space. Using Latin hypercube sampling (LHS), I maximise the distance between my samples, and minimise the correlations between predictor variables, creating a uniform distribution of sampling across all axes of predictor space. This is way more efficient than the factorial design, but has required some changes to my parallelisation script:

```{R LHS, eval = FALSE}
# Get environment variables

TMPDIR <- Sys.getenv('TMPDIR')
USER <- Sys.getenv('USER')


# Parallelisation libraries 

library(foreach)
library(doParallel)
library(future)


# Seed generation - 100 seeds from the total range of possible values in a 32 bit signed integer

rsample <- sample(1:2147483647, 100)


# Load LHS samples

lscombos <- read.csv(paste0("/home/",USER,"/SLiM/Scripts/lscombos150.csv"), header = T)

# - run on all available cores, treat them as nodes

cl <- makeCluster(future::availableCores())
registerDoParallel(cl)

#Run SLiM, defining parameter sets according to LHS in command line

foreach(i=rsample) %:%
  foreach(j=1:nrow(lscombos)) %dopar% {
    # Use string manipulation functions to configure the command line args, feeding from a data frame of latin square combinations
    # then run SLiM with system(),
    slim_out <- system(sprintf("/home/$USER/SLiM/slim -d seed=%i -d rregion=%s -d Ne=%i -d QTL_cov=%f -d pleiorate=%f -d delmu=%f -d rwide=%s -d modelindex=%i /home/$USER/SLiM/Scripts/neutral2T10GRjune.slim", 
                               i, as.character(lscombos$rregions[j]), as.integer(round(lscombos$Ne[j])), lscombos$pleiocov[j], lscombos$pleiorate[j], lscombos$delmu[j], as.character(lscombos$rwide[j]), j, intern=T))
  }
stopCluster(cl)
```

I now only have too foreach loops: one for the seeds (which are replicates of the same model), and one for each combination of parameters, given by a table, lscombos. So far I've been generating 100 seeds and 100 LHS combinations, however both of these will likely have to increase as there have been some gaps in the sampling space when looking at the minute differences between parameters (due to not enough combinations, or models), and the seeds represent within-model variance due to differences in starting parameters, so getting a better idea of the within-model variance will improve our understanding of the uncertainty.
Also in here is my modelindex, which is just the combination that's being used (as explained earlier on).

I'm using the library DoE.wrapper to generate my latin cubes, which in itself contains the packages DiceDesign and LHS. This package makes it a little nicer to use, allowing for arbitrary ranges and scales for each parameter, which is useful since most of my parameters have different scales: e.g. population sizes being between 10 and 10000, likely much larger in the future, and recombination rates genome-wide being 1e-8 - 1e-6.
Here is the code:
```{R LHS2, eval = FALSE}
# Generate latin hypercube sample for all the factors

library(DoE.wrapper)

lscombos <- lhs.design(
  nruns = 150, # 100 is too few, but this drastically increases the amount of runs: nruns * nseeds is the total. Also generating the LHC takes forever with more of these
  nfactors = 6,
  seed = 1370976025, #sampled from 1:2147483647
  #  type = "improved",
  factor.names = list(
    rregions = c(0.0, 0.5),
    Ne = c(10, 10000),
    pleiocov = c(-0.5, 0.5), # More even sampling, going from -0.5 to 0.5 rather than 0 to 0.5
    pleiorate = c(0.0, 1.0),
    delmu = c(0.0, 1.0),
    rwide = c(0.0, 1e-6)) # Try it, don't know how long it will run!
  
)
```

There is a fairly large problem with this I have noticed so far however: When increasing the number of runs, the time to calculate all the combinations increases very quickly (exponentially, in fact). Since each combination is relative to each other one (maximising distance, minimising correlation), you can't really parallelise this process very well, so I'll need to find some way to run this well - a fast single-core solution will likely be necessary, although I'm not sure how much improved it can get - I'm sure an i9 or Ryzen 9 overclocked to infinity will do very well, but those are expensive solutions, and I don't know if that would be fast enough with nruns = 500 or something like that. 250 takes ~12 hours to generate on my 4770.

I've now gone into a bit of a different direction - looking at generating populations of models to characterise the null space in terms of phenotype. The idea is to use LHS to sample across realistic ranges of parameters such as deleterious mutation rates, population sizes, pleiotropic covariance, pleiotropy rates, recombination rates, linkage etc. (all the parameters I am looking at). This entire sample is a population of models which together describes how a population may evolve under neutral drift, with each sample representing a specific genetic architecture. By matchign experimental data to this population phenotypic data, we can determine if that trait/traits is/are a null case (evolving under genetic drift), or if something else is going on. By seeing whereabouts in the population of models the experimental data sits also gives an estimate of the genetic architecture underlying that/those trait/traits. Building a population of models under different forms of selection will extend this to allow for identification of strengths and directions of selection etc.
This is definitely too in-depth to be done in just this Honours project, but I can make a start and hopefully end up with a set of null models that I can use in future to compare to actual trait data.

The first thing I can think of complicating this is my models currently don't take into account environmental deviations and genotype*environment interactions. I'll have to think carefully about modelling these, and read into how environmental noise can be modelled. Perhaps heritability should be a parameter in the models, so if we have that information for experimental data we can use it to narrow down what models are most accurate.


## Conclusions

Lots of interesting developments over the past month or so - I've moved on to two traits, further improved my models and have a new focus on populations of models. My next steps are to run some models on Tinaroo, implement environmental deviations (perhaps), and figure out LHC analysis - I'll likely have to group my continuous parameters into ranges and compare those groups.

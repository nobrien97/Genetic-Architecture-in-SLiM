---
title: "Burn-In"
author: "Nick O'Brien"
date: "23/07/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

I've been having some problems setting up the burn in for my simulations. The premise is that at mutation-drift equilibrium, the population's mean heterozygosity should be relatively stable, as predicted by population genetics theory. The time to burn-in should be proportional to 4 times the population size, and the predicted heterozygosity is equal to 4Nu. This is assuming completely neutral drift. 

However, I've found I have been approaching the problem from the wrong angle: I need to be looking at the cyclical nature of heterozygosity as would be expected under mutation-drift balance (or mutation-drift-selection balance). Although on average over many seeds, you would expect the mean population heterozygosity to reach 4Nu, for any given seed, it is unlikely to stay at that point over time exactly: rather, mutations accumulate to push heterozygosity higher, and then drift results in fixation or loss, bringing the heterozygosity lower again. This results in a cycle (almost like a sine or cosine wave) that have amplitude and frequency proportional to population size, mutation rate, and other parameters - recombination, deleterious mutation, etc.

Here, I examine this cyclical action with several population sizes in order to better understand the signature of a population in mutation-drift balance, and choose appropriate points to assume a population is burnt in (given thir population size).

To see this cyclical nature, we can take an autocorrelation plot, or a figure of the population mean heterozygosity over time. Here is an example of population size = 1000:

![Autocorrelation of population mean heterozygosity: Ne - 1000](Z:/Documents/GitHub/Genetic-Architecture-in-SLiM/src/SLiM/autocorr_Ne1000_example.png)
![Population mean heterozygosity: Ne - 1000](Z:/Documents/GitHub/Genetic-Architecture-in-SLiM/src/SLiM/meanH_Ne1000_example.png)

As you can see, there is a cycle of autocorrelation spikes that appear every so often, however their amplitude gets smaller over time (as you get further away from the initial time point). This is a signature of mutation-drift equilibrium. The two dotted lines in the means figure represent 5% either side of the expected 4Nu value of heterozygosity at equilibrium. If across all seeds and all time points we got the mean of this data, we would find that we do get around that value, however the wave behaviour of heterozygosity at equilibrium over time does not allow that at all time points (and due to the stochasticity of drift, not at all seeds).
In fact, this mean (after 20,000 generations, or well after that initial climb to equilibrium from the empty state) does fall within there, at 0.0259, very close to the prediction of 0.0252 (4Nu = 4 * 1000 * 6.3e-6).

## Further justification for burn-in times

In this document I show the autocorrelations of different population sizes, deleterious mutation rates, and recombination rates to get a feel for how long I should leave my burn-ins before running my models.
I expect that deleterious mutations (in the absence of recombination) will likely affect the amplitude of the wave greatly, but I am unsure of how it will affect the time to reach this wave-state (which is the important part for ensuring we are in some kind of balance).


## Null model

![Population mean heterozygosity: Ne - 1000](Z:/Documents/GitHub/Genetic-Architecture-in-SLiM/src/SLiM/meanheterozygosity_test.png)

Looks like population size affects time to equilibrium as expected! Recombination rate didn't (although I only had two levels: 0 and 1e-5). Deleterious mutation does some weird stuff: drastically lowers the equilibrium value, although an equilibrium does appear to be reached, and quite quickly as well across all population sizes. As if deleterious mutation is stifling the ability of neutral mutations to rise in frequency, and puts a cap on how much can segregate at a time.
It is important then that we make sure deleterious mutations are kept in from the outset: if we leave them out for burn in, and introduce them later, we will be creating a second burn in during our run as the heterozygosity drops to the levels seen in the bottom figure. Given that there still is an equilibrium, it should be fine. It does reach equilibrium faster however, so there is potential to save some time by using a shorter burn-in for different values of deleterious mutation prevalence.


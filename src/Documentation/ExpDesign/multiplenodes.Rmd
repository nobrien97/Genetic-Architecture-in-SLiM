---
title: "Working with Multiple Nodes on Tinaroo"
author: "Nick O'Brien"
date: "13/07/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction
Though I initially thought it was simple, moving to several nodes on a HPC isn't super straightforward apparently.
My initial thought was to let R do the work and extend my initial core-wise parallelisation to a second layer of node-wise parallelisation using future, foreach, and doParallel. This led to this code:

``` {r cluster, eval=FALSE}
makeCluster(future::availableWorkers(methods = "PBS"))
```

Which doesn't work. I tried some more ways of feeding the PBS node data to R by querying the PBS environment variable $PBS_NODEFILE, however this also did not work. In fact, every attempt to use R packages to parallelise over nodes did not work for me. So, I went to an alternative method, using a job array to split my job over multiple subjobs (20, to be precise), with each subjob being assigned to a separate node. This led to a revamping of my entire way of doing things in my .pbs script, and a few changes to my foreach loop, but my R script stayed largely the same. Now, my R script handles by-core parallelisation, with by-node parallelisation being handled by the PBS system on Tinaroo.

## The R script

``` {r, rscript, eval=FALSE}
##############################################################################################################
#  Simple test script for including two traits, with arguments coming from latin square, for use on cluster
##############################################################################################################


#  Parallel script modified from SLiM-Extras example R script, info at
#  the SLiM-Extras repository at https://github.com/MesserLab/SLiM-Extras.

# Get environment variables

TMPDIR <- Sys.getenv('TMPDIR')
USER <- Sys.getenv('USER')
PBSIND <- as.numeric(Sys.getenv("PBS_ARRAY_INDEX"))

print(PBSIND)


# Parallelisation libraries 

library(foreach)
library(doParallel)
library(future)

# Load LHS samples - Split into 20 parts, so each node does 5 each (500 total runs/node)

lscombos <- read.csv(paste0("/home/",USER,"/SLiM/Scripts/lscombos.csv"), header = T)

rsample <- read.csv(paste0("/home/",USER,"/SLiM/Scripts/seeds.csv"), header = F)
rsample <- as.character(as.vector(t(rsample)))

# - Multinode parallelism: Use the array index to tell us what range in lscombos to evaluate: from (index*5) -4: index*5

cl <- makeCluster(future::availableCores())
registerDoParallel(cl)

#Run SLiM, defining parameter sets according to LHS in command line - j is from (PBSIND*5 )-4 to PBSIND*5

foreach(i=rsample) %:%
  foreach(j=((PBSIND*5) - 4):(PBSIND*5)) %dopar% {
    # Use string manipulation functions to configure the command line args, feeding from a data frame of latin square combinations
    # then run SLiM with system(),
    slim_out <- system(sprintf("/home/$USER/SLiM/slim -d seed=%s -d rregion=%s -d Ne=%i -d QTL_cov=%f -d pleiorate=%f -d delmu=%f -d rwide=%s -d modelindex=%i /home/$USER/SLiM/Scripts/null2T100L_june_restruct2_opt.slim", 
                               as.character(i), as.character(lscombos$rregions[j]), as.integer(round(lscombos$Ne[j])), lscombos$pleiocov[j], lscombos$pleiorate[j], lscombos$delmu[j], as.character(lscombos$rwide[j]), j, intern=T))
  }
stopCluster(cl)

```


So, to break this down: first, the script gets two important environment variables: the temporary directory, user directory, and most importantly, the array index. This is the number assigned to the subjob: e.g. if I had an array of 1-20 jobs, then the 10th subjob is job[10]. I use this number to determine which part of the lscombos the node will run: each node runs 5 of the 100 total hypercube combinations, and all 100 replicates of those 5. Since we are taking 5 jobs, we multiply the array index by that number for the maximum index of lscombos we will reach, and take 4 from that for the minimum. (seen in the j= ... line in the foreach loop). TMPDIR and USER are used for file paths.

That handles the per-core parallelisation for each node, but the PBS script handles splitting the job across several nodes:

## The PBS Script

```{bash, pbsscript, eval=FALSE}
#!/bin/bash -l
#PBS -q workq
#PBS -A qris-uq
#PBS -N slim_null2T_array
#PBS -J 1-20
#PBS -l walltime=110:00:00
#PBS -l select=1:ncpus=24:mem=120G

cd $TMPDIR

MAX=$((PBS_ARRAY_INDEX*5))

MIN=$((PBS_ARRAY_INDEX*5-4))

mkdir -p $(seq --format './matrices/model%.0f' $MIN $MAX)

module load R/3.5.0

echo "Array index = $PBS_ARRAY_INDEX"

R --file=/home/$USER/SLiM/Scripts/2traits_jobarray_unix.R

zip -r /30days/$USER/matrices_${PBS_ARRAY_INDEX}.zip /$TMPDIR/matrices

cat /$TMPDIR/out_2T100L_means.csv >> /30days/$USER/out_2T100L_means.csv

cat /$TMPDIR/out_2T100L_muts.csv >> /30days/$USER/out_2T100L_muts.csv
```

The usual PBS commands are first: we define a queue to go in, the account name for Tinaroo, a name for the job, an allocated amount of time for the job, and a number of nodes, cores, and memory **for each subjob**! The -j parameter determines that the job is a job array, and what the indices used for the array should be: here I used indices 1 to 20, for 20 total subjobs, on 20 different nodes. Each subjob will run this script. First, we go to the temporary directory so we write all our output data in a reasonable place. We then determine the minimum and maximum indices of our data frame of LHC samples (i.e. the actual LHC combos that the subjob will run). The script creates folders named after these indices to store our matrices in, mainly so we don't overwrite our matrices all the time. After that, we load R, run our script, then zip our matrix output and append our means and mutation data to the bottom of our master file in our final output directory.  



